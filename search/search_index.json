{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the WebMEV documentation page. This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Home"},{"location":"#welcome-to-the-webmev-documentation-page","text":"This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Welcome to the WebMEV documentation page."},{"location":"api/","text":"Documentation on the API There are a couple aspects to the WevMEV REST API. We have the actual API endpoints which are used to drive analysis or a frontend visualization. Documentation of the API interaction is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation Behind the API itself are the data structures and models that we use to architect the system. You will find information about these entities and their relationships in this section.","title":"Intro"},{"location":"api/#documentation-on-the-api","text":"There are a couple aspects to the WevMEV REST API. We have the actual API endpoints which are used to drive analysis or a frontend visualization. Documentation of the API interaction is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation Behind the API itself are the data structures and models that we use to architect the system. You will find information about these entities and their relationships in this section.","title":"Documentation on the API"},{"location":"attributes/","text":"Attributes Attribute s could also be thought of as \"parameters\" and are a way of providing validated key-value pairs. The different types enforce various constraints on the underlying primitive type (e.g. a float bounded between [0,1] can represent a probability). Mainly, Attribute s are a way to add information to Observation or Feature instances. For example, one could specify the phenotype or experimental group of an Observation via a StringAttribute . class api.data_structures.attributes. BaseAttribute ( value , **kwargs ) Base object which defines some common methods and members for Attribute types Classes that derive from BaseAttribute have strings which identify their type ( typename ) and a value , which is specific to the child class implementation. See child classes for examples. class api.data_structures.attributes. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttribute and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class api.data_structures.attributes. IntegerAttribute ( value , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class api.data_structures.attributes. PositiveIntegerAttribute ( value , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class api.data_structures.attributes. NonnegativeIntegerAttribute ( value , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class api.data_structures.attributes. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class api.data_structures.attributes. FloatAttribute ( value , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } class api.data_structures.attributes. PositiveFloatAttribute ( value , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } class api.data_structures.attributes. NonnegativeFloatAttribute ( value , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } class api.data_structures.attributes. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class api.data_structures.attributes. StringAttribute ( value , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } api.data_structures.attributes. create_attribute ( attr_key , attribute_dict ) Utility function used by the serializers to create/return BaseAttribute-derived instances. Accepts an attribute_dict which is a Python dictionary object containing the keys appropriate to create a particular attribute. For example, to create a BoundedIntegerAttribute , this dict would be formatted as, attr_dict = { 'attribute_type': 'BoundedInteger', 'value': 3, 'min': 0, 'max': 10 }","title":"Attributes"},{"location":"attributes/#attributes","text":"Attribute s could also be thought of as \"parameters\" and are a way of providing validated key-value pairs. The different types enforce various constraints on the underlying primitive type (e.g. a float bounded between [0,1] can represent a probability). Mainly, Attribute s are a way to add information to Observation or Feature instances. For example, one could specify the phenotype or experimental group of an Observation via a StringAttribute . class api.data_structures.attributes. BaseAttribute ( value , **kwargs ) Base object which defines some common methods and members for Attribute types Classes that derive from BaseAttribute have strings which identify their type ( typename ) and a value , which is specific to the child class implementation. See child classes for examples. class api.data_structures.attributes. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttribute and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class api.data_structures.attributes. IntegerAttribute ( value , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class api.data_structures.attributes. PositiveIntegerAttribute ( value , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class api.data_structures.attributes. NonnegativeIntegerAttribute ( value , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class api.data_structures.attributes. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class api.data_structures.attributes. FloatAttribute ( value , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } class api.data_structures.attributes. PositiveFloatAttribute ( value , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } class api.data_structures.attributes. NonnegativeFloatAttribute ( value , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } class api.data_structures.attributes. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class api.data_structures.attributes. StringAttribute ( value , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } api.data_structures.attributes. create_attribute ( attr_key , attribute_dict ) Utility function used by the serializers to create/return BaseAttribute-derived instances. Accepts an attribute_dict which is a Python dictionary object containing the keys appropriate to create a particular attribute. For example, to create a BoundedIntegerAttribute , this dict would be formatted as, attr_dict = { 'attribute_type': 'BoundedInteger', 'value': 3, 'min': 0, 'max': 10 }","title":"Attributes"},{"location":"auth/","text":"Authentication with MEV Once a user is registered (with an email and password), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication"},{"location":"auth/#authentication-with-mev","text":"Once a user is registered (with an email and password), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication with MEV"},{"location":"creating_analyses/","text":"Creating a new analysis ( Operation ) for use with WebMEV WebMEV analyses (AKA Operation s) are designed to be transparent and portable. While certain files are required for integration with the WebMEV application, the analyses are designed so that they are self-contained and can be transparently reproduced elsewhere. Depending on the nature of the analysis, jobs are either executed locally (on the WebMEV server) or remotely on ephemeral hardware that is dynamically provisioned from the cloud computing provider. Thus, the \"run mode\" of the analyses affects which files are required for WebMEV integration. Below, we describe the architecture of WebMEV-compatible analyses and how one can go about creating new ones. Local Docker-based mode Typically, local Docker-based jobs are used for lightweight analyses that require a minimal amount of hardware. Examples include principal-component analyses, differential expression testing, and other script-based jobs with relatively modest footprints. Local Docker-based jobs are intended to be invoked like a standard commandline executable or script. Specifically, to run the job, we start the Docker container with a command similar to: docker run -d -v <docker volume>:<container workspace> --entrypoint=<CMD> <IMAGE> This runs the command specified by <CMD> in the environment provided by the Docker image. In this way, we isolate the software dependencies of the analysis from the host system and provide users a way to recreate their analysis at a later time, or to independently clone the analysis repository and run it on any Docker-capable system. To construct a WebMEV-compatible analysis for local-Docker execution mode, we require the following files be present in a repository: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . converters.json This file tells WebMEV how to take a user-supplied input (e.g. a list of samples/ Observation s) and format it for the commandline invocation (e.g. as a comma-delimited list of strings) entrypoint.txt This is a text file that provides a command template to be filled-in with the appropriate concrete arguments. The templating syntax is Jinja2 (https://jinja.palletsprojects.com) docker/Dockerfile The docker folder contains (at minimum) a Dockerfile which provides the \"recipe\" for building the Docker image. Additional files to be included in the Docker build context (such as scripts) can be placed in this folder. Outputs While there are no restrictions on the nature or content of the analysis itself, we have to capture the analysis outputs in a manner that WebMEV can interpret those outputs and present them to end-users. Thus, we require that the process create an outputs.json file in the container's \"workspace\". This file is accessible to WebMEV via the shared volume provided with the -v argument to the docker run command. More details below in the concrete example. Note that this is the only place where analysis code makes any reference to WebMEV. However, the creation of an outputs.json file does not influence the analysis code in any manner-- one could take an existing script, add a few lines to create the outputs.json and it would be ready for use as an analysis module in WebMEV. Example For this example, we look at the requirements for a simple principal component analysis (PCA). The repository is available at https://github.com/web-mev/pca/ We first describe the overall structure and then talk specifically about each file. Overall structure The repository has: - operation_spec.json (required) - converters.json (required) - entrypoint.txt (required) - docker/ - Dockerfile (required) - run_pca.py - requirements.txt The analysis is designed so that it will execute a single python script ( docker/run_pca.py ) as follows: run_pca.py -i <path to input file> [-s <comma-delimited list of sample names>] The first arg ( -i ) provides a path to an input matrix (typically an expression/abundance matrix). The second (optional) argument ( -s ) allows us to specify sample names to use, formatted as a comma-delimited list of samples. By default (if no argument provided) all samples are used. In addition to running the PCA, this script will also create the outputs.json file. It's not required that you structure the code in any particular manner, but the analysis has to create the outputs.json file at some point before the container exits. Otherwise, the results will not be accessible for display with WebMEV. docker/ folder and Docker context In the docker/ folder we have the required Dockerfile , the script to run ( run_pca.py ), and a requirements.txt file which provides the packages needed to construct the proper Python installation. The Dockerfile looks like: FROM debian:stretch RUN apt-get update && \\ apt-get install -y python3-dev python3-pip # Install some Python3 libraries: RUN mkdir /opt/software ADD requirements.txt /opt/software/ ADD run_pca.py /opt/software/ RUN chmod +x /opt/software/run_pca.py RUN pip3 install -r /opt/software/requirements.txt ENTRYPOINT [\"/opt/software/run_pca.py\"] requirements.txt looks like: (truncated) cryptography==1.7.1 ... scikit-learn==0.22.2.post1 ... scipy==1.4.1 ... run_pca.py : For brevity, we omit the full run_pca.py script (available at https://github.com/web-mev/pca/blob/master/docker/run_pca.py), but note that the Dockerfile places this script in the /opt/software folder. Thus, we have to either append to the PATH in the container, or provide the full path to this script when we invoke it for execution. Below (see entrypoint.txt ) we use the latter. Finally, we note that this script creates an outputs.json file: ... outputs = { 'pca_coordinates': <path to output matrix of principal coordinates>, 'pc1_explained_variance':pca.explained_variance_ratio_[0], 'pc2_explained_variance': pca.explained_variance_ratio_[1] } json.dump(outputs, open(os.path.join(working_dir, 'outputs.json'), 'w')) As stated prior, it's not required that this script create that file, but that the file be created at some point before the container exits. This is the only place where scripts are required to \"know about WebMEV\". Everything else in the script operates divorced from any notion of WebMEV architecture. operation_spec.json The operation_spec.json file provides a description of the analysis and follows the format of our Operation data structure: { \"name\": \"\", \"description\": \"\", \"inputs\": <Mapping of keys to OperationInput objects>, \"outputs\": <Mapping of keys to OperationOutput objects>, \"mode\": \"\" } Importantly, the mode key must be set to \"local_docker\" which lets WebMEV know that this analysis/ Operation will be run as a Docker-based process on the server. Failure to provide a valid value for this key will trigger an error when the analysis is \"ingested\" and prepared by WebMEV. Concretely our PCA analysis: { \"name\": \"Principal component analysis (PCA)\", \"description\": \"Executes a 2-d PCA to examine the structure and variation of a dataset.\", \"inputs\": { \"input_matrix\": { \"description\": \"The input matrix. For example, a gene expression matrix for a cohort of samples.\", \"name\": \"Input matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"MTX\",\"I_MTX\", \"EXP_MTX\", \"RNASEQ_COUNT_MTX\"], \"many\": false } }, \"samples\": { \"description\": \"The samples to use in the PCA. By default, it will use all samples/observations.\", \"name\": \"Samples:\", \"required\": false, \"spec\": { \"attribute_type\": \"ObservationSet\" } } }, \"outputs\": { \"pca_coordinates\": { \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"MTX\", \"many\": false } }, \"pc1_explained_variance\": { \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } }, \"pc2_explained_variance\": { \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } } }, \"mode\": \"local_docker\" } In the inputs section, this Operation states that it has one required ( input_matrix ) and one optional input ( samples ). For input_matrix , we expect a single input file (a DataResource with many=false ) that has an appropriate resource type. As PCA requires a numeric matrix (in our convention, with samples/observations in columns and genes/features in rows) we restrict these input types to one of \"MTX\" , \"I_MTX\" , \"EXP_MTX\" , or \"RNASEQ_COUNT_MTX\" . The full list of all resource types is available at /api/resource-types/ The second, optional input ( samples ) allows us to subset the columns of the matrix to only include samples/observations of interest. The specification of this input states that we must provide it with an object of type ObservationSet . Recall, however, that our script is invoked by providing a comma-delimited list of sample names to the -s argument. Thus, we will use one of the \"converter\" classes to convert the ObservationSet instance into a comma-delimited string. This choice is left up to the developer of the analysis-- one could very well choose to provde the ObservationSet instance as an argument to their script and parse that accordingly. For outputs, we expect a single DataResource with type \"MTX\" and two bounded floats, which represent the explained variance of the PCA. converters.json The converters.json file tells WebMEV how to take a user-input and convert it to the appropriate format to invoke the script. To accomplish this, we provide a mapping of the input \"name\" to a class which implements the conversion. For us, this looks like: { \"input_matrix\":\"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"samples\": \"api.converters.element_set.ObservationSetCsvConverter\" } The class implementations are provided using Python's \"dotted\" notation. A variety of commonly-used converters are provided with WebMEV, but developers are free to create their own implementations for their own WebMEV instances. The only requirement is that the class implements a common interface method named convert , which takes the user-supplied input and returns an appropriate output. As an example, we note that the LocalDockerSingleDataResourceConverter above takes the user-supplied input (a UUID which identifies a file/ Resource they own) and \"converts\" it to a path on the server. In this way, the run_pca.py script is not concerned with how WebMEV stores files, etc. WebMEV handles the file moving/copying and analyses can be written without dependencies on WebMEV-related architecture or how files are stored within WebMEV. The samples input uses the api.converters.element_set.ObservationSetCsvConverter which converts an ObservationSet such as: { \"multiple\": true, \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } into a CSV string: sampleA,sampleB . Clearly, this requires some knowledge of the available \"converter\" implementations. We expect that there are not so many that this burden is unreasonable. We decided that the flexibility provided by this inconvenience was more beneficial than restricting the types of inputs and how they can be formatted for invoking jobs. entrypoint.txt The entrypoint file has the command that will be run as the ENTRYPOINT of the Docker container. To accommodate optional inputs and permit additional flexibility, we use jinja2 template syntax. In our example, we have: /opt/software/run_pca.py -i {{input_matrix}} {% if samples %} -s {{samples}} {% endif %} (as referenced above, note that we provide the full path to the Python script. Alternatively, we could put the script somewhere on the PATH when building the Docker image) The variables in this template (between the double braces) must match the keys provided in the inputs section of the operation_spec.json document. Thus, if the samples input is omitted (which means all samples are used in the PCA calculation), the final command would look like: /opt/software/run_pca.py -i <path to matrix> If the samples input is provided, WebMEV handles converting the ObservationSet instance into a comma-delimited string to create: /opt/software/run_pca.py -i <path to matrix> -s A,B,C (e.g. for samples/observations named \"A\", \"B\", and \"C\") A suggested workflow for creating new analyses First, without consideration for WebMEV, consider the expected inputs and outputs of your analysis. Generally, this will be some combination of files and simple parameters like strings or numbers. Now, write this hypothetical analysis as a formal Operation into the operation_spec.json file. Create a Dockerfile and corresponding Docker image with and an analysis script that is executable as a simple commandline program. Take care to include some code to create the outputs.json file at some point in the process. Take the \"prototype\" command you would use to execute the script and write it into entrypoint.txt using jinja2 template syntax. The input variable keys should correspond to those in your operation_spec.json . Create the converters.json file which will reformat the inputs into items that the entrypoint command will understand. Once all these files are in place, create a git repository and check the code into github. The analysis is ready for ingestion with WebMEV. Remote, Cromwell-based jobs TODO","title":"Creating new analyses/operations"},{"location":"creating_analyses/#creating-a-new-analysis-operation-for-use-with-webmev","text":"WebMEV analyses (AKA Operation s) are designed to be transparent and portable. While certain files are required for integration with the WebMEV application, the analyses are designed so that they are self-contained and can be transparently reproduced elsewhere. Depending on the nature of the analysis, jobs are either executed locally (on the WebMEV server) or remotely on ephemeral hardware that is dynamically provisioned from the cloud computing provider. Thus, the \"run mode\" of the analyses affects which files are required for WebMEV integration. Below, we describe the architecture of WebMEV-compatible analyses and how one can go about creating new ones.","title":"Creating a new analysis (Operation) for use with WebMEV"},{"location":"creating_analyses/#local-docker-based-mode","text":"Typically, local Docker-based jobs are used for lightweight analyses that require a minimal amount of hardware. Examples include principal-component analyses, differential expression testing, and other script-based jobs with relatively modest footprints. Local Docker-based jobs are intended to be invoked like a standard commandline executable or script. Specifically, to run the job, we start the Docker container with a command similar to: docker run -d -v <docker volume>:<container workspace> --entrypoint=<CMD> <IMAGE> This runs the command specified by <CMD> in the environment provided by the Docker image. In this way, we isolate the software dependencies of the analysis from the host system and provide users a way to recreate their analysis at a later time, or to independently clone the analysis repository and run it on any Docker-capable system. To construct a WebMEV-compatible analysis for local-Docker execution mode, we require the following files be present in a repository: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . converters.json This file tells WebMEV how to take a user-supplied input (e.g. a list of samples/ Observation s) and format it for the commandline invocation (e.g. as a comma-delimited list of strings) entrypoint.txt This is a text file that provides a command template to be filled-in with the appropriate concrete arguments. The templating syntax is Jinja2 (https://jinja.palletsprojects.com) docker/Dockerfile The docker folder contains (at minimum) a Dockerfile which provides the \"recipe\" for building the Docker image. Additional files to be included in the Docker build context (such as scripts) can be placed in this folder. Outputs While there are no restrictions on the nature or content of the analysis itself, we have to capture the analysis outputs in a manner that WebMEV can interpret those outputs and present them to end-users. Thus, we require that the process create an outputs.json file in the container's \"workspace\". This file is accessible to WebMEV via the shared volume provided with the -v argument to the docker run command. More details below in the concrete example. Note that this is the only place where analysis code makes any reference to WebMEV. However, the creation of an outputs.json file does not influence the analysis code in any manner-- one could take an existing script, add a few lines to create the outputs.json and it would be ready for use as an analysis module in WebMEV.","title":"Local Docker-based mode"},{"location":"creating_analyses/#example","text":"For this example, we look at the requirements for a simple principal component analysis (PCA). The repository is available at https://github.com/web-mev/pca/ We first describe the overall structure and then talk specifically about each file. Overall structure The repository has: - operation_spec.json (required) - converters.json (required) - entrypoint.txt (required) - docker/ - Dockerfile (required) - run_pca.py - requirements.txt The analysis is designed so that it will execute a single python script ( docker/run_pca.py ) as follows: run_pca.py -i <path to input file> [-s <comma-delimited list of sample names>] The first arg ( -i ) provides a path to an input matrix (typically an expression/abundance matrix). The second (optional) argument ( -s ) allows us to specify sample names to use, formatted as a comma-delimited list of samples. By default (if no argument provided) all samples are used. In addition to running the PCA, this script will also create the outputs.json file. It's not required that you structure the code in any particular manner, but the analysis has to create the outputs.json file at some point before the container exits. Otherwise, the results will not be accessible for display with WebMEV. docker/ folder and Docker context In the docker/ folder we have the required Dockerfile , the script to run ( run_pca.py ), and a requirements.txt file which provides the packages needed to construct the proper Python installation. The Dockerfile looks like: FROM debian:stretch RUN apt-get update && \\ apt-get install -y python3-dev python3-pip # Install some Python3 libraries: RUN mkdir /opt/software ADD requirements.txt /opt/software/ ADD run_pca.py /opt/software/ RUN chmod +x /opt/software/run_pca.py RUN pip3 install -r /opt/software/requirements.txt ENTRYPOINT [\"/opt/software/run_pca.py\"] requirements.txt looks like: (truncated) cryptography==1.7.1 ... scikit-learn==0.22.2.post1 ... scipy==1.4.1 ... run_pca.py : For brevity, we omit the full run_pca.py script (available at https://github.com/web-mev/pca/blob/master/docker/run_pca.py), but note that the Dockerfile places this script in the /opt/software folder. Thus, we have to either append to the PATH in the container, or provide the full path to this script when we invoke it for execution. Below (see entrypoint.txt ) we use the latter. Finally, we note that this script creates an outputs.json file: ... outputs = { 'pca_coordinates': <path to output matrix of principal coordinates>, 'pc1_explained_variance':pca.explained_variance_ratio_[0], 'pc2_explained_variance': pca.explained_variance_ratio_[1] } json.dump(outputs, open(os.path.join(working_dir, 'outputs.json'), 'w')) As stated prior, it's not required that this script create that file, but that the file be created at some point before the container exits. This is the only place where scripts are required to \"know about WebMEV\". Everything else in the script operates divorced from any notion of WebMEV architecture. operation_spec.json The operation_spec.json file provides a description of the analysis and follows the format of our Operation data structure: { \"name\": \"\", \"description\": \"\", \"inputs\": <Mapping of keys to OperationInput objects>, \"outputs\": <Mapping of keys to OperationOutput objects>, \"mode\": \"\" } Importantly, the mode key must be set to \"local_docker\" which lets WebMEV know that this analysis/ Operation will be run as a Docker-based process on the server. Failure to provide a valid value for this key will trigger an error when the analysis is \"ingested\" and prepared by WebMEV. Concretely our PCA analysis: { \"name\": \"Principal component analysis (PCA)\", \"description\": \"Executes a 2-d PCA to examine the structure and variation of a dataset.\", \"inputs\": { \"input_matrix\": { \"description\": \"The input matrix. For example, a gene expression matrix for a cohort of samples.\", \"name\": \"Input matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"MTX\",\"I_MTX\", \"EXP_MTX\", \"RNASEQ_COUNT_MTX\"], \"many\": false } }, \"samples\": { \"description\": \"The samples to use in the PCA. By default, it will use all samples/observations.\", \"name\": \"Samples:\", \"required\": false, \"spec\": { \"attribute_type\": \"ObservationSet\" } } }, \"outputs\": { \"pca_coordinates\": { \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"MTX\", \"many\": false } }, \"pc1_explained_variance\": { \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } }, \"pc2_explained_variance\": { \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } } }, \"mode\": \"local_docker\" } In the inputs section, this Operation states that it has one required ( input_matrix ) and one optional input ( samples ). For input_matrix , we expect a single input file (a DataResource with many=false ) that has an appropriate resource type. As PCA requires a numeric matrix (in our convention, with samples/observations in columns and genes/features in rows) we restrict these input types to one of \"MTX\" , \"I_MTX\" , \"EXP_MTX\" , or \"RNASEQ_COUNT_MTX\" . The full list of all resource types is available at /api/resource-types/ The second, optional input ( samples ) allows us to subset the columns of the matrix to only include samples/observations of interest. The specification of this input states that we must provide it with an object of type ObservationSet . Recall, however, that our script is invoked by providing a comma-delimited list of sample names to the -s argument. Thus, we will use one of the \"converter\" classes to convert the ObservationSet instance into a comma-delimited string. This choice is left up to the developer of the analysis-- one could very well choose to provde the ObservationSet instance as an argument to their script and parse that accordingly. For outputs, we expect a single DataResource with type \"MTX\" and two bounded floats, which represent the explained variance of the PCA. converters.json The converters.json file tells WebMEV how to take a user-input and convert it to the appropriate format to invoke the script. To accomplish this, we provide a mapping of the input \"name\" to a class which implements the conversion. For us, this looks like: { \"input_matrix\":\"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"samples\": \"api.converters.element_set.ObservationSetCsvConverter\" } The class implementations are provided using Python's \"dotted\" notation. A variety of commonly-used converters are provided with WebMEV, but developers are free to create their own implementations for their own WebMEV instances. The only requirement is that the class implements a common interface method named convert , which takes the user-supplied input and returns an appropriate output. As an example, we note that the LocalDockerSingleDataResourceConverter above takes the user-supplied input (a UUID which identifies a file/ Resource they own) and \"converts\" it to a path on the server. In this way, the run_pca.py script is not concerned with how WebMEV stores files, etc. WebMEV handles the file moving/copying and analyses can be written without dependencies on WebMEV-related architecture or how files are stored within WebMEV. The samples input uses the api.converters.element_set.ObservationSetCsvConverter which converts an ObservationSet such as: { \"multiple\": true, \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } into a CSV string: sampleA,sampleB . Clearly, this requires some knowledge of the available \"converter\" implementations. We expect that there are not so many that this burden is unreasonable. We decided that the flexibility provided by this inconvenience was more beneficial than restricting the types of inputs and how they can be formatted for invoking jobs. entrypoint.txt The entrypoint file has the command that will be run as the ENTRYPOINT of the Docker container. To accommodate optional inputs and permit additional flexibility, we use jinja2 template syntax. In our example, we have: /opt/software/run_pca.py -i {{input_matrix}} {% if samples %} -s {{samples}} {% endif %} (as referenced above, note that we provide the full path to the Python script. Alternatively, we could put the script somewhere on the PATH when building the Docker image) The variables in this template (between the double braces) must match the keys provided in the inputs section of the operation_spec.json document. Thus, if the samples input is omitted (which means all samples are used in the PCA calculation), the final command would look like: /opt/software/run_pca.py -i <path to matrix> If the samples input is provided, WebMEV handles converting the ObservationSet instance into a comma-delimited string to create: /opt/software/run_pca.py -i <path to matrix> -s A,B,C (e.g. for samples/observations named \"A\", \"B\", and \"C\")","title":"Example"},{"location":"creating_analyses/#a-suggested-workflow-for-creating-new-analyses","text":"First, without consideration for WebMEV, consider the expected inputs and outputs of your analysis. Generally, this will be some combination of files and simple parameters like strings or numbers. Now, write this hypothetical analysis as a formal Operation into the operation_spec.json file. Create a Dockerfile and corresponding Docker image with and an analysis script that is executable as a simple commandline program. Take care to include some code to create the outputs.json file at some point in the process. Take the \"prototype\" command you would use to execute the script and write it into entrypoint.txt using jinja2 template syntax. The input variable keys should correspond to those in your operation_spec.json . Create the converters.json file which will reformat the inputs into items that the entrypoint command will understand. Once all these files are in place, create a git repository and check the code into github. The analysis is ready for ingestion with WebMEV.","title":"A suggested workflow for creating new analyses"},{"location":"creating_analyses/#remote-cromwell-based-jobs","text":"TODO","title":"Remote, Cromwell-based jobs"},{"location":"elements/","text":"Elements, Observations, and Features We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from BaseElement , which captures their common structure. Specialization specific to each can be overridden in the child classes. In an experimental context, Observation s are analogous to samples. That is, each Observation has one or more Feature s associated with it (e.g. gene expressions for 30,000 genes). Collectively, we can think of Observation s and Feature s as comprising the rows and columns of a two-dimensional matrix. We use Observation s and Feature s to hold metadata about data that we manipulating in MEV. We can attach attributes to these to allow users to set experimental groups, or other information usedful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class api.data_structures.element. BaseElement ( id , attribute_dict={} ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity, we keep those entities separate. Yet, their behavior and structure are very much the same. This also allows us to add custom behavior to each at a later time if we require. An Element is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.observation. Observation ( id , attribute_dict={} ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.feature. Feature ( id , attribute_dict={} ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Observations and Features"},{"location":"elements/#elements-observations-and-features","text":"We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from BaseElement , which captures their common structure. Specialization specific to each can be overridden in the child classes. In an experimental context, Observation s are analogous to samples. That is, each Observation has one or more Feature s associated with it (e.g. gene expressions for 30,000 genes). Collectively, we can think of Observation s and Feature s as comprising the rows and columns of a two-dimensional matrix. We use Observation s and Feature s to hold metadata about data that we manipulating in MEV. We can attach attributes to these to allow users to set experimental groups, or other information usedful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class api.data_structures.element. BaseElement ( id , attribute_dict={} ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity, we keep those entities separate. Yet, their behavior and structure are very much the same. This also allows us to add custom behavior to each at a later time if we require. An Element is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.observation. Observation ( id , attribute_dict={} ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.feature. Feature ( id , attribute_dict={} ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Elements, Observations, and Features"},{"location":"example_workflow/","text":"Example workflow To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Workflow and analysis concepts"},{"location":"example_workflow/#example-workflow","text":"To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Example workflow"},{"location":"install/","text":"Installation instructions Dockerhub image TODO Local build The application is packaged as a series of Docker images for easier management of dependencies. The collective behavior and interdependencies is managed by docker-compose . To build the application image yourself, you will need to have Docker and docker-compose installed locally. To build the application, clone the repository locally: git clone https://github.com/web-mev/mev-backend.git To run the container, you will need to supply some environment variables for things like passwords and other sensitive information. In the root of the repository is a file named env_vars.template.txt . Fill that in with the various usernames and passwords as you like. Each variable has a description to help. In particular, note the following: DJANGO_ALLOWED_HOSTS : If you are deploying on a host that is not your own computer (e.g. on a cloud machine), supply the IP address or domain. DJANGO_CORS_ORIGINS : If you are using a front-end framework located on a different domain, you will need to add this to the comma-delimited list. Otherwise you will get failures due to violoating same-origin policy. RESOURCE_STORAGE_BACKEND : This configures where user files are stored. By default, it is local. Other storage backends may be configured (i.e. for cloud bucket-based storage). Given as a \"dotted\" path to the storage class implementation. EMAIL_BACKEND_CHOICE : Email-based registration depends on the ability to send emails, so a viable email backend must be chosen. Refer to the settings file to see available options. Currently only GMAIL . SOCIAL_BACKENDS : A list of social auth providers you wish to use. Currently only Google. If you would like some \"dummy\" data to be entered into the database (e.g. for developing a front-end), you must also specify POPULUATE_DB=yes (case-sensitive!). Any other values for this variable will skip the population of dummy data. For more information on configuration, see Configuration To start the application in development mode: In development mode, the application server (gunicorn) does not start automatically as the container starts. Rather, all the containers are started but the application container ( api ) remains idle. This allows us to mount a local directory where we can dynamically edit the code and immediately see changes. Note that the Django DEBUG argument is set to True in this case, so be mindful of using development mode if the server is exposed to the public. In the root of the repository (where the docker-compose.yml file resides) run: docker-compose up -d --build This builds all the containers and runs in \"detached\" mode. By default, docker-compose will look for the docker-compose.yml file which, by design, puts us in development mode. The current directory on the host machine will be mounted at /workspace inside the api container. Next, enter the container with: docker-compose exec api /bin/bash You may optionally choose to add the -u 0 flag which logs you into the api container as root. Once inside, run the following: cd /workspace/mev source startup.sh This will run some database migrations and other preliminaries, but will not actually start the gunicorn application server. To do that, you must then run: cd /workspace/mev gunicorn mev.wsgi:application --bind 0.0.0.0:8000 (note the cd at the top since the startup.sh script ends up moving you into the /www directory). The bind argument should have the port set to 8000 as this is how the NGINX container communicates over the internal docker-compose network. Following all that, the application should be running on port 8081 (e.g. http://127.0.0.1:8081/api/) If you are interested, note that additional gunicorn configuration parameters can be specified (see https://docs.gunicorn.org/en/latest/configure.html). By stopping the gunicorn server (Ctrl+C), you can make local edits to your code (again, connected into the container via the volume mount) and immediately restart the server to see the changes. The unit test suite can also be run in this manner with python3 /workspace/mev/manage.py test To start the application in production mode: In production mode, the application server will be started following the usual startup steps contained in startup.sh . In the root of the repository (where the docker-compose.prod.yml file resides) run: docker-compose -f docker-compose.prod.yml up -d --build This should start everything up. On occasion, if you are very quick to navigate to the site, NGINX will issue a 502 bad gateway error. However, a refresh should open the site correctly. In production mode, Django debugging is turned off, so any errors will be reported as generic 500 server errors without any corresponding details. Stopping the application To shut down the application (in verbose mode), run docker-compose down -v","title":"Installation"},{"location":"install/#installation-instructions","text":"","title":"Installation instructions"},{"location":"install/#dockerhub-image","text":"TODO","title":"Dockerhub image"},{"location":"install/#local-build","text":"The application is packaged as a series of Docker images for easier management of dependencies. The collective behavior and interdependencies is managed by docker-compose . To build the application image yourself, you will need to have Docker and docker-compose installed locally. To build the application, clone the repository locally: git clone https://github.com/web-mev/mev-backend.git To run the container, you will need to supply some environment variables for things like passwords and other sensitive information. In the root of the repository is a file named env_vars.template.txt . Fill that in with the various usernames and passwords as you like. Each variable has a description to help. In particular, note the following: DJANGO_ALLOWED_HOSTS : If you are deploying on a host that is not your own computer (e.g. on a cloud machine), supply the IP address or domain. DJANGO_CORS_ORIGINS : If you are using a front-end framework located on a different domain, you will need to add this to the comma-delimited list. Otherwise you will get failures due to violoating same-origin policy. RESOURCE_STORAGE_BACKEND : This configures where user files are stored. By default, it is local. Other storage backends may be configured (i.e. for cloud bucket-based storage). Given as a \"dotted\" path to the storage class implementation. EMAIL_BACKEND_CHOICE : Email-based registration depends on the ability to send emails, so a viable email backend must be chosen. Refer to the settings file to see available options. Currently only GMAIL . SOCIAL_BACKENDS : A list of social auth providers you wish to use. Currently only Google. If you would like some \"dummy\" data to be entered into the database (e.g. for developing a front-end), you must also specify POPULUATE_DB=yes (case-sensitive!). Any other values for this variable will skip the population of dummy data. For more information on configuration, see Configuration To start the application in development mode: In development mode, the application server (gunicorn) does not start automatically as the container starts. Rather, all the containers are started but the application container ( api ) remains idle. This allows us to mount a local directory where we can dynamically edit the code and immediately see changes. Note that the Django DEBUG argument is set to True in this case, so be mindful of using development mode if the server is exposed to the public. In the root of the repository (where the docker-compose.yml file resides) run: docker-compose up -d --build This builds all the containers and runs in \"detached\" mode. By default, docker-compose will look for the docker-compose.yml file which, by design, puts us in development mode. The current directory on the host machine will be mounted at /workspace inside the api container. Next, enter the container with: docker-compose exec api /bin/bash You may optionally choose to add the -u 0 flag which logs you into the api container as root. Once inside, run the following: cd /workspace/mev source startup.sh This will run some database migrations and other preliminaries, but will not actually start the gunicorn application server. To do that, you must then run: cd /workspace/mev gunicorn mev.wsgi:application --bind 0.0.0.0:8000 (note the cd at the top since the startup.sh script ends up moving you into the /www directory). The bind argument should have the port set to 8000 as this is how the NGINX container communicates over the internal docker-compose network. Following all that, the application should be running on port 8081 (e.g. http://127.0.0.1:8081/api/) If you are interested, note that additional gunicorn configuration parameters can be specified (see https://docs.gunicorn.org/en/latest/configure.html). By stopping the gunicorn server (Ctrl+C), you can make local edits to your code (again, connected into the container via the volume mount) and immediately restart the server to see the changes. The unit test suite can also be run in this manner with python3 /workspace/mev/manage.py test To start the application in production mode: In production mode, the application server will be started following the usual startup steps contained in startup.sh . In the root of the repository (where the docker-compose.prod.yml file resides) run: docker-compose -f docker-compose.prod.yml up -d --build This should start everything up. On occasion, if you are very quick to navigate to the site, NGINX will issue a 502 bad gateway error. However, a refresh should open the site correctly. In production mode, Django debugging is turned off, so any errors will be reported as generic 500 server errors without any corresponding details. Stopping the application To shut down the application (in verbose mode), run docker-compose down -v","title":"Local build"},{"location":"metadata/","text":"About Workspace metadata As described elsewhere, all analyses occur in the context of a user's Workspace ; the Workspace allows users to organize files and analyses logically. To operate on the potentially multiple files contained in a user's Workspace , we are obligated to track metadata that spans across the data resources and is maintained at the level of the user's Workspace . We are typically required to provide this metadata to analyses/ Operation s, including information about samples ( Observation s), genes ( Feature s), and possibly other annotation data. A Workspace can have multiple file/ Resource objects associated with it, each of which has its own unique metadata. Therefore, we conceive of \"workspace metadata\" which is composed from the union of the individual Resource 's metadata. Consider two Resource instances in a Workspace . The first ( Resource \"A\") is data generated by a user and has six samples, which we will denote as S1,...,S6; S1-S3 are wild-type and S4-S6 are mutant. The ObservationSet associated with Resource A could look like: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\" } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\" } } ] } The other ( Resource B) is public-domain data and also has six samples, which we will denote as P1,...,P6. The ObservationSet associated with Resource B could look like: { \"multiple\": true, \"elements\": [ { \"id\": \"P1\", \"attributes\": {} }, ... { \"id\": \"P6\", \"attributes\": {} } ] } (Note that for simplicity/brevity these samples don't have any annotations/attributes for this example). Now, as far as the Workspace is concerned, there are 12 Observation instances by performing a union of the Observation s contained in the ObservationSet associated with each Resource . In the course of performing an analysis, the user might wish to create meaningful \"groups\" of these samples. Perhaps they merge the two count matrices underlying Resource s A and B, and perform a principal component analysis (PCA) on the output. They then note a clustering of the samples which they perceive as meaningful: (Via the dynamic user interface, we imagine the user selecting the five samples in the grey ellipse-- two of the public \"P\" samples, P3 and P4 cluster with the WT samples). They can then choose to create a new ObservationSet from those five samples: { \"multiple\": true, \"elements\": [ { \"id\": \"S4\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} } ] } This information regarding user-defined groupings can be cached client-side. However, the ultimate \"source\" of the metadata is provided by the Workspace , which maintains the ObservationSet s, FeatureSet s, and possibly other metadata. We could use a heatmap to visualize how users have created various selections: Using the metadata for analyses After the user has created their own ObservationSet instances, they can use them for analyses such as a differential expression analysis. For instance, the inputs to such an analysis would be an expression matrix (perhaps the result of merging the \"S\" and \"P\" samples/ Observation s) and two ObservationSet instances. The payload to start such an analysis (sent to /api/operations/run/ ) would look something like: { \"operation_id\": <UUID for Operation>, \"workspace_id\": <UUID for Workspace>, \"inputs\": { \"count_matrix\": <UUID for merged Resource>, \"groupA\": <ObservationSet with S4,S5,S6,P3,P4>, \"groupB\": <ObservationSet with S1,S2,S3,P5,P6> } } Additional user-supplied metadata Finally, the users might want to add additional annotations to their metadata. For instance, assuming we still are working with Resource instances A and B, we could upload an additional Resource with type \"ANN\" (for ann otation) and add it to this Workspace . For instance, maybe it looks like: sample sex p53_mutant_status S1 M 1 S2 F 0 S3 F 0 S4 F 1 S5 M 1 S6 M 0 This would then incorporate into the existing Observation instances so they now would look like: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 0 } } ] } Unsure how this might work on the front-end in terms of updating the Observation instances so that they all have this information. If the ObservationSet was setup to have pointers/references to the Observation instances then the changes could percolate to all the ObservationSet instances. Backend endpoints To provide a \"single source of truth\", there will be a \"workspace metadata\" endpoint at /api/workspace/<UUID>/metadata/ which will track the union of all the Resource metadata in the Workspace . To reduce the amount of data returned, there will also be specific endpoints for Observation s and Feature s at /api/workspace/<UUID>/metadata/observations/ and /api/workspace/<UUID>/metadata/features/ . The front-end will maintain the various user selections (formerly \"sample sets\", now ObservationSet ) but the full set of available Observation instances will be kept on the backend. Using the example above, a request to /api/workspace/<UUID>/metadata/observations/ would return: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, { \"id\": \"S2\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"F\", \"p53_mutant_status\": 0 } }, { \"id\": \"S3\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"F\", \"p53_mutant_status\": 0 } }, { \"id\": \"S4\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"F\", \"p53_mutant_status\": 1 } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 0 } }, { \"id\": \"P1\", \"attributes\": {} }, { \"id\": \"P2\", \"attributes\": {} }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} }, { \"id\": \"P5\", \"attributes\": {} }, { \"id\": \"P6\", \"attributes\": {} }, ] }","title":"Metadata"},{"location":"metadata/#about-workspace-metadata","text":"As described elsewhere, all analyses occur in the context of a user's Workspace ; the Workspace allows users to organize files and analyses logically. To operate on the potentially multiple files contained in a user's Workspace , we are obligated to track metadata that spans across the data resources and is maintained at the level of the user's Workspace . We are typically required to provide this metadata to analyses/ Operation s, including information about samples ( Observation s), genes ( Feature s), and possibly other annotation data. A Workspace can have multiple file/ Resource objects associated with it, each of which has its own unique metadata. Therefore, we conceive of \"workspace metadata\" which is composed from the union of the individual Resource 's metadata. Consider two Resource instances in a Workspace . The first ( Resource \"A\") is data generated by a user and has six samples, which we will denote as S1,...,S6; S1-S3 are wild-type and S4-S6 are mutant. The ObservationSet associated with Resource A could look like: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\" } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\" } } ] } The other ( Resource B) is public-domain data and also has six samples, which we will denote as P1,...,P6. The ObservationSet associated with Resource B could look like: { \"multiple\": true, \"elements\": [ { \"id\": \"P1\", \"attributes\": {} }, ... { \"id\": \"P6\", \"attributes\": {} } ] } (Note that for simplicity/brevity these samples don't have any annotations/attributes for this example). Now, as far as the Workspace is concerned, there are 12 Observation instances by performing a union of the Observation s contained in the ObservationSet associated with each Resource . In the course of performing an analysis, the user might wish to create meaningful \"groups\" of these samples. Perhaps they merge the two count matrices underlying Resource s A and B, and perform a principal component analysis (PCA) on the output. They then note a clustering of the samples which they perceive as meaningful: (Via the dynamic user interface, we imagine the user selecting the five samples in the grey ellipse-- two of the public \"P\" samples, P3 and P4 cluster with the WT samples). They can then choose to create a new ObservationSet from those five samples: { \"multiple\": true, \"elements\": [ { \"id\": \"S4\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} } ] } This information regarding user-defined groupings can be cached client-side. However, the ultimate \"source\" of the metadata is provided by the Workspace , which maintains the ObservationSet s, FeatureSet s, and possibly other metadata. We could use a heatmap to visualize how users have created various selections:","title":"About Workspace metadata"},{"location":"metadata/#using-the-metadata-for-analyses","text":"After the user has created their own ObservationSet instances, they can use them for analyses such as a differential expression analysis. For instance, the inputs to such an analysis would be an expression matrix (perhaps the result of merging the \"S\" and \"P\" samples/ Observation s) and two ObservationSet instances. The payload to start such an analysis (sent to /api/operations/run/ ) would look something like: { \"operation_id\": <UUID for Operation>, \"workspace_id\": <UUID for Workspace>, \"inputs\": { \"count_matrix\": <UUID for merged Resource>, \"groupA\": <ObservationSet with S4,S5,S6,P3,P4>, \"groupB\": <ObservationSet with S1,S2,S3,P5,P6> } }","title":"Using the metadata for analyses"},{"location":"metadata/#additional-user-supplied-metadata","text":"Finally, the users might want to add additional annotations to their metadata. For instance, assuming we still are working with Resource instances A and B, we could upload an additional Resource with type \"ANN\" (for ann otation) and add it to this Workspace . For instance, maybe it looks like: sample sex p53_mutant_status S1 M 1 S2 F 0 S3 F 0 S4 F 1 S5 M 1 S6 M 0 This would then incorporate into the existing Observation instances so they now would look like: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 0 } } ] } Unsure how this might work on the front-end in terms of updating the Observation instances so that they all have this information. If the ObservationSet was setup to have pointers/references to the Observation instances then the changes could percolate to all the ObservationSet instances.","title":"Additional user-supplied metadata"},{"location":"metadata/#backend-endpoints","text":"To provide a \"single source of truth\", there will be a \"workspace metadata\" endpoint at /api/workspace/<UUID>/metadata/ which will track the union of all the Resource metadata in the Workspace . To reduce the amount of data returned, there will also be specific endpoints for Observation s and Feature s at /api/workspace/<UUID>/metadata/observations/ and /api/workspace/<UUID>/metadata/features/ . The front-end will maintain the various user selections (formerly \"sample sets\", now ObservationSet ) but the full set of available Observation instances will be kept on the backend. Using the example above, a request to /api/workspace/<UUID>/metadata/observations/ would return: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, { \"id\": \"S2\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"F\", \"p53_mutant_status\": 0 } }, { \"id\": \"S3\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"F\", \"p53_mutant_status\": 0 } }, { \"id\": \"S4\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"F\", \"p53_mutant_status\": 1 } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 0 } }, { \"id\": \"P1\", \"attributes\": {} }, { \"id\": \"P2\", \"attributes\": {} }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} }, { \"id\": \"P5\", \"attributes\": {} }, { \"id\": \"P6\", \"attributes\": {} }, ] }","title":"Backend endpoints"},{"location":"observation_and_feature_sets/","text":"ObservationSet and FeatureSet We can compose unique sets of Observation and Feature instances into ObservationSet and FeatureSet instances, respectively. These data structures can be used as a way to subset/filter samples/observations or to provide groups of samples to analyses (such as when comparing two groups for a differential expression analysis). class api.data_structures.observation_set. ObservationSet ( init_elements , multiple=True ) An ObservationSet is a collection of unique Observation instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the ObservationSet is the set of samples that were assayed. We depend on the native python set data structure and appropriately hashable/comparable Observation instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Observation s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"multiple\": <bool>, \"elements\": [ <Observation>, <Observation>, ... ] } class api.data_structures.feature_set. FeatureSet ( init_elements , multiple=True ) A FeatureSet is a collection of unique Feature instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the FeatureSet is the set of genes. We depend on the native python set data structure and appropriately hashable/comparable Feature instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Feature s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"multiple\": <bool>, \"elements\": [ <Feature>, <Feature>, ... ] }","title":"ObservationSet and FeatureSets"},{"location":"observation_and_feature_sets/#observationset-and-featureset","text":"We can compose unique sets of Observation and Feature instances into ObservationSet and FeatureSet instances, respectively. These data structures can be used as a way to subset/filter samples/observations or to provide groups of samples to analyses (such as when comparing two groups for a differential expression analysis). class api.data_structures.observation_set. ObservationSet ( init_elements , multiple=True ) An ObservationSet is a collection of unique Observation instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the ObservationSet is the set of samples that were assayed. We depend on the native python set data structure and appropriately hashable/comparable Observation instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Observation s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"multiple\": <bool>, \"elements\": [ <Observation>, <Observation>, ... ] } class api.data_structures.feature_set. FeatureSet ( init_elements , multiple=True ) A FeatureSet is a collection of unique Feature instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the FeatureSet is the set of genes. We depend on the native python set data structure and appropriately hashable/comparable Feature instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Feature s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"multiple\": <bool>, \"elements\": [ <Feature>, <Feature>, ... ] }","title":"ObservationSet and FeatureSet"},{"location":"operations/","text":"Operations and ExecutedOperations An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines orchestrated using the CNAP-style workflows involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. See below for a concrete example. Outputs of the analysis. This would be similar to the inputs in that it describes the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a \"feature table\" ( FT , FeatureTable ) giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type \"expression matrix\" ( EXP_MTX ). github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. The front-end users don't need to know that, but internally MEV needs to know how to run the analysis. Note that some of these will be specified by whoever creates the analysis. However, some information (like the UUID identifier, git hash, etc.) will be populated when the analysis is \"ingested\". It should not be the job of the analysis developer to maintain those pieces of information and we can create them on the fly during ingestion. Therefore, an Operation has the following structure: { \"id\": <UUID>, \"name\": <string>, \"description\": <string>, \"inputs\": Object<OperationInput>, \"outputs\": Object<OperationOutput>, \"mode\": <string>, \"repository\": <string url>, \"git_hash\": <string> } where: - mode : identifies how the analysis is run. Will be one of an enum of strings (e.g. local_docker , cromwell ) - repository : identifies the github repo used to pull the data. For the ingestion, admins will give that url which will initiate a clone process. - git_hash : This is the commit hash which uniquely identifies the code state. This way the analysis code can be exactly traced back. Both inputs and outputs address nested objects. That is, they are mappings of string identifiers to OperationInput or OperationOutput instances: { 'abc': <OperationInput/OperationOutput>, 'def': <OperationInput/OperationOutput> } An OperationInput looks like: { \"description\": <string>, \"name\": <string>, \"required\": <bool>, \"spec\": <InputSpec> } (and similarly for OperationOutput , which has fewer keys). As an example of an OperationInputs , consider a p-value for thresholding: { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 } } The spec key addresses a child class of InputSpec whose behavior is specific to each \"type\" (above, a BoundedFloat ). There are only a limited number of those so defining a set of options for each is straightforward. ExecutedOperation s should maintain the following data: The Workspace (which also gives access to the user/owner) a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier. We need the Cromwell job UUID to track the progress as we query the Cromwell server. For Docker-based jobs, we can set the tag on the container and then query its status (e.g. if it's still \"up\", then the job is still going) The inputs to the analysis (a JSON document) The outputs (another JSON document) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings A concrete example For this, consider a differential expression analysis (e.g. like DESeq2). In this simplified analysis, we will take a count matrix, a p-value (for filtering significance based on some hypothesis test), and an output file name. For outputs, we have a single file which has the results of the differential expression testing on each gene. Since each row concerns a gene (and the columns give information about that gene), the output file is a \"feature table\" in our nomenclature. Thus, the file which defines this analysis would look like: { \"name\":\"DESeq2 differential gene expression\", \"description\": \"Find genes which are differentially expressed and filter...\" \"inputs\": { \"count_matrix\": { \"description\": \"The count matrix of expressions\", \"name\": \"Count matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"RNASEQ_COUNT_MTX\", \"I_MTX\"], \"many\": false } }, \"p_val\": { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 }, } \"output_filename\": { \"description\": \"The name of the contrast for your own reference.\", \"name\": \"Contrast name:\", \"required\": false, \"spec\": { \"attribute_type\": \"String\", \"default\": \"deseq2_results\" } } }, \"outputs\": { \"dge_table\": { \"spec\":{ \"attribute_type\": \"DataResource\", \"resource_type\": \"FT\" } } } } This specification will be placed into a file. In the repo, there will be a Dockerfile and possibly other files (e.g. scripts). Upon ingestion, MEV will read this inputs file, get the commit hash, assign a UUID, build the container, push the container, etc. As mentioned before, we note that the JSON above does not contain all of the required fields to create an Operation instance; it is missing id , git_hash , and repository_url . Note that when the API endpoint /api/operations/ is requested, the returned object will match that above, but will also contain those required additional fields. Executing an Operation The Operation objects above are typically used to populate the user interface such that the proper input fields can be displayed (e.g. a file chooser for an input that specifies it requires a DataResource ). To actually initiate the Operation , thus creating an ExecutedOperation , the front-end (or API request) will need to POST a payload with the proper parameters/inputs. The backend will check those inputs against the specification. As an example, a valid payload for the above would be: { \"operation_id\": <UUID>, \"workspace_id\": <UUID>, \"inputs\": { \"count_matrix\": <UUID of Resource> \"p_val\": 0.01 } } (note that the \"output_filename\" field was not required so we do not need it here) The operation_id allows us to locate the Operation that we wish to run and the workspace_id allows us to associate the eventual ExecutedOperation with a Workspace . Finally, the inputs key is an object of key-value pairs. Depending on the \"type\" of the input, the values can be effectively arbitrary. Walking through the backend logic In the backend, we locate the proper Operation by its UUID. In our example, we see that this Operation expects two required inputs: \"count_matrix\" and \"p_val\" . Below, we walk though how these are validated. For the count_matrix input, we see the spec field says it accepts \"DataResource\" s (files) with resource types of [\"RNASEQ_COUNT_MTX\", \"I_MTX\"] . It also says \"many\": false , so we only will accept a single file. The payload example above provided a single UUID (so it is validated for \"many\": false ). Then, we will take that UUID and query our database to see if it corresponds to a Resource instance that has a resource_type member that is either \"RNASEQ_COUNT_MTX\" or \"I_MTX\" . If that is indeed the case, then the \"count_matrix\" field is successfully validated. For the \"p_val\" field, we receive a value of 0.01. The spec states that this input should be of type BoundedFloat with a min of 0.0 and a max of 1.0. The backend validates that 0.01 is indeed in the range [0.0,1.0]. Operation modes Depending on how the Operation should be run, we perform different actions upon ingestion. For local docker-based runs, we obviously require that the image be located on the same machine as the MEV instance. To ensure everything is \"aligned\", we require additional files depending on the run mode. These are verified during the ingestion process. For instance, in the local docker run mode, we need a Dockerfile to build from. The image will be Given that all the inputs successfully validate, we can move on to create an ExecutedOperation and actually run the analysis.","title":"Operation concepts"},{"location":"operations/#operations-and-executedoperations","text":"An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines orchestrated using the CNAP-style workflows involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. See below for a concrete example. Outputs of the analysis. This would be similar to the inputs in that it describes the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a \"feature table\" ( FT , FeatureTable ) giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type \"expression matrix\" ( EXP_MTX ). github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. The front-end users don't need to know that, but internally MEV needs to know how to run the analysis. Note that some of these will be specified by whoever creates the analysis. However, some information (like the UUID identifier, git hash, etc.) will be populated when the analysis is \"ingested\". It should not be the job of the analysis developer to maintain those pieces of information and we can create them on the fly during ingestion. Therefore, an Operation has the following structure: { \"id\": <UUID>, \"name\": <string>, \"description\": <string>, \"inputs\": Object<OperationInput>, \"outputs\": Object<OperationOutput>, \"mode\": <string>, \"repository\": <string url>, \"git_hash\": <string> } where: - mode : identifies how the analysis is run. Will be one of an enum of strings (e.g. local_docker , cromwell ) - repository : identifies the github repo used to pull the data. For the ingestion, admins will give that url which will initiate a clone process. - git_hash : This is the commit hash which uniquely identifies the code state. This way the analysis code can be exactly traced back. Both inputs and outputs address nested objects. That is, they are mappings of string identifiers to OperationInput or OperationOutput instances: { 'abc': <OperationInput/OperationOutput>, 'def': <OperationInput/OperationOutput> } An OperationInput looks like: { \"description\": <string>, \"name\": <string>, \"required\": <bool>, \"spec\": <InputSpec> } (and similarly for OperationOutput , which has fewer keys). As an example of an OperationInputs , consider a p-value for thresholding: { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 } } The spec key addresses a child class of InputSpec whose behavior is specific to each \"type\" (above, a BoundedFloat ). There are only a limited number of those so defining a set of options for each is straightforward. ExecutedOperation s should maintain the following data: The Workspace (which also gives access to the user/owner) a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier. We need the Cromwell job UUID to track the progress as we query the Cromwell server. For Docker-based jobs, we can set the tag on the container and then query its status (e.g. if it's still \"up\", then the job is still going) The inputs to the analysis (a JSON document) The outputs (another JSON document) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings","title":"Operations and ExecutedOperations"},{"location":"operations/#a-concrete-example","text":"For this, consider a differential expression analysis (e.g. like DESeq2). In this simplified analysis, we will take a count matrix, a p-value (for filtering significance based on some hypothesis test), and an output file name. For outputs, we have a single file which has the results of the differential expression testing on each gene. Since each row concerns a gene (and the columns give information about that gene), the output file is a \"feature table\" in our nomenclature. Thus, the file which defines this analysis would look like: { \"name\":\"DESeq2 differential gene expression\", \"description\": \"Find genes which are differentially expressed and filter...\" \"inputs\": { \"count_matrix\": { \"description\": \"The count matrix of expressions\", \"name\": \"Count matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"RNASEQ_COUNT_MTX\", \"I_MTX\"], \"many\": false } }, \"p_val\": { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 }, } \"output_filename\": { \"description\": \"The name of the contrast for your own reference.\", \"name\": \"Contrast name:\", \"required\": false, \"spec\": { \"attribute_type\": \"String\", \"default\": \"deseq2_results\" } } }, \"outputs\": { \"dge_table\": { \"spec\":{ \"attribute_type\": \"DataResource\", \"resource_type\": \"FT\" } } } } This specification will be placed into a file. In the repo, there will be a Dockerfile and possibly other files (e.g. scripts). Upon ingestion, MEV will read this inputs file, get the commit hash, assign a UUID, build the container, push the container, etc. As mentioned before, we note that the JSON above does not contain all of the required fields to create an Operation instance; it is missing id , git_hash , and repository_url . Note that when the API endpoint /api/operations/ is requested, the returned object will match that above, but will also contain those required additional fields.","title":"A concrete example"},{"location":"operations/#executing-an-operation","text":"The Operation objects above are typically used to populate the user interface such that the proper input fields can be displayed (e.g. a file chooser for an input that specifies it requires a DataResource ). To actually initiate the Operation , thus creating an ExecutedOperation , the front-end (or API request) will need to POST a payload with the proper parameters/inputs. The backend will check those inputs against the specification. As an example, a valid payload for the above would be: { \"operation_id\": <UUID>, \"workspace_id\": <UUID>, \"inputs\": { \"count_matrix\": <UUID of Resource> \"p_val\": 0.01 } } (note that the \"output_filename\" field was not required so we do not need it here) The operation_id allows us to locate the Operation that we wish to run and the workspace_id allows us to associate the eventual ExecutedOperation with a Workspace . Finally, the inputs key is an object of key-value pairs. Depending on the \"type\" of the input, the values can be effectively arbitrary. Walking through the backend logic In the backend, we locate the proper Operation by its UUID. In our example, we see that this Operation expects two required inputs: \"count_matrix\" and \"p_val\" . Below, we walk though how these are validated. For the count_matrix input, we see the spec field says it accepts \"DataResource\" s (files) with resource types of [\"RNASEQ_COUNT_MTX\", \"I_MTX\"] . It also says \"many\": false , so we only will accept a single file. The payload example above provided a single UUID (so it is validated for \"many\": false ). Then, we will take that UUID and query our database to see if it corresponds to a Resource instance that has a resource_type member that is either \"RNASEQ_COUNT_MTX\" or \"I_MTX\" . If that is indeed the case, then the \"count_matrix\" field is successfully validated. For the \"p_val\" field, we receive a value of 0.01. The spec states that this input should be of type BoundedFloat with a min of 0.0 and a max of 1.0. The backend validates that 0.01 is indeed in the range [0.0,1.0].","title":"Executing an Operation"},{"location":"operations/#operation-modes","text":"Depending on how the Operation should be run, we perform different actions upon ingestion. For local docker-based runs, we obviously require that the image be located on the same machine as the MEV instance. To ensure everything is \"aligned\", we require additional files depending on the run mode. These are verified during the ingestion process. For instance, in the local docker run mode, we need a Dockerfile to build from. The image will be Given that all the inputs successfully validate, we can move on to create an ExecutedOperation and actually run the analysis.","title":"Operation modes"},{"location":"resource_metadata/","text":"Resource metadata Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). The former is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Thus, associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent. We maintain a \"master copy\" of the metadata on the server side as a flat file for reference. We do not want to have to repeatedly open/parse a large text file to determine the rows/features and columns/observations. We imagine that for performance reasons, client-applications may choose to cache this metadata so that desired sets of rows or columns can be selected on the client side without involving a request to the server. Requests to subset/filter a DataResource would provide ObservationSet s or FeatureSet s which are compared against the respective ObservationSet s or FeatureSet s of the DataResource .","title":"Resource metadata"},{"location":"resource_metadata/#resource-metadata","text":"Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). The former is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Thus, associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent. We maintain a \"master copy\" of the metadata on the server side as a flat file for reference. We do not want to have to repeatedly open/parse a large text file to determine the rows/features and columns/observations. We imagine that for performance reasons, client-applications may choose to cache this metadata so that desired sets of rows or columns can be selected on the client side without involving a request to the server. Requests to subset/filter a DataResource would provide ObservationSet s or FeatureSet s which are compared against the respective ObservationSet s or FeatureSet s of the DataResource .","title":"Resource metadata"},{"location":"resource_types/","text":"Resource types A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. The string identifiers map to concrete classes that implement validation methods for the Resource . When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into two broad categories: Table-based formats Sequence-based formats Table-based formats are any array-like format, such as a typical CSV file. This covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. The primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM. Table-based resource types class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Special tab-delimited files like BED or VCF files are recognized by their canonical extension (e.g. \".bed\" or \".vcf\"). Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to create Attribute s which can be added to the Observation s. After the annotations are uploaded, the users must tell MEV how to interpret the columns (e.g. as a string? as a bounded float?), but once that type is specified, we can validate the annotations against that choice and subsequently add the Attribute s to the Observation s. class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here. Sequence-based formats class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for validating Fasta files, compressed or not. Fasta files are recognized using the following formats: fasta fasta.gz fa fa.gz class resource_types.sequence_types. FastQResource ( ) This resource type is for Fastq files, compressed or not. Fastq files are recognized using the following formats: fastq fastq.gz fq fq.gz class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files. We accept both SAM and BAM files named using their canonical extensions: \".bam\" for BAM files \".sam\" for SAM files","title":"Resource types"},{"location":"resource_types/#resource-types","text":"A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. The string identifiers map to concrete classes that implement validation methods for the Resource . When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into two broad categories: Table-based formats Sequence-based formats Table-based formats are any array-like format, such as a typical CSV file. This covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. The primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM.","title":"Resource types"},{"location":"resource_types/#table-based-resource-types","text":"class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Special tab-delimited files like BED or VCF files are recognized by their canonical extension (e.g. \".bed\" or \".vcf\"). Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to create Attribute s which can be added to the Observation s. After the annotations are uploaded, the users must tell MEV how to interpret the columns (e.g. as a string? as a bounded float?), but once that type is specified, we can validate the annotations against that choice and subsequently add the Attribute s to the Observation s. class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here.","title":"Table-based resource types"},{"location":"resource_types/#sequence-based-formats","text":"class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for validating Fasta files, compressed or not. Fasta files are recognized using the following formats: fasta fasta.gz fa fa.gz class resource_types.sequence_types. FastQResource ( ) This resource type is for Fastq files, compressed or not. Fastq files are recognized using the following formats: fastq fastq.gz fq fq.gz class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files. We accept both SAM and BAM files named using their canonical extensions: \".bam\" for BAM files \".sam\" for SAM files","title":"Sequence-based formats"},{"location":"resources/","text":"Resources Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders. They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. Resource \"type\" A Resource is required to have a \"type\" (e.g. an integer matrix) which we call a resource_type . These types are restricted to a set of common file formats. Upon creation, resource_type is set to None which indicates that the Resource has not been validated. The type of the Resource can be specified when the file is uploaded or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. If the validation of the resource_type fails, we revert back to the previous successfully validated type. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. Admins can, however, specify a Workspace in their request to create the Resource directly via the API. When a user chooses to \"add\" a Resource to a Workspace , a new database record is created which is a copy of the original, unattached Resource with the same attributes except the unique Resource UUID. Thus, we have two database records referencing the same file. We could accomplish something similar with a many-to-one mapping of Workspace to Resource s, but this was a choice we made which could allow for resource-copying if we ever allow file-editing in the future. In that case, attaching a Resource to a Workspace could create a copy of the file such that the original Resource remains unaltered. The user can, of course, change any of the mutable members of this new Workspace -associated Resource . The changes will be independent of the original \"unattached\" Resource . Users can remove a Resource from a Workspace if it has NOT been used for any portions of the analysis. We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Deletion of Resources Since multiple database records can reference the same underlying file, we have a bit of custom logic for determining when we delete only the database record versus deleting the actual underlying file. Essentially, if a deletion is requested and no other Resource database records reference the same file, then we delete both the database record AND the file. In the case where there is another database record referencing that file, we only remove the database record, leaving the file. Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). Some additional notes: Resource s are owned by users and can be added to a Workspace . However, that is not required-- Resource s can be \"unattached\". Regular users (non-admins) can't create new Resource directly via the API. The only way they can create a Resource is indirectly by adding a new upload. When a Resource is added to a Workspace , a new copy of the database record is made. This maintains the state of the original Resource . Resource s can be made \"public\" so that others can view and import them. Once another user chooses to import the file, a copy is made and that new user has their own copy. If a Resource is later made \"private\" then any files that have been \"used\" by others cannot be recalled. Resource s can be removed from a Workspace , but only if they have not been used for any analyses/operations. Resource s cannot be transferred from one Workspace to another, but they can be copied. A change in the type of the Resource can be requested. Until the validation of that change is complete, the Resource is made private and inactive. Admins can make essentially any change to Resources , including creation. However, they must be careful to maintain the integrity of the database and the files they point to. In a request to create a Resource via the API, the resource_type field can be blank/null. The type can be inferred from the path of the resource. We can do this because only admins are allowed to create via the API and they should only generate such requests if the resource type can be inferred (i.e. admins know not to give bad requests to the API...)","title":"General info"},{"location":"resources/#resources","text":"Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders. They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. Resource \"type\" A Resource is required to have a \"type\" (e.g. an integer matrix) which we call a resource_type . These types are restricted to a set of common file formats. Upon creation, resource_type is set to None which indicates that the Resource has not been validated. The type of the Resource can be specified when the file is uploaded or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. If the validation of the resource_type fails, we revert back to the previous successfully validated type. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. Admins can, however, specify a Workspace in their request to create the Resource directly via the API. When a user chooses to \"add\" a Resource to a Workspace , a new database record is created which is a copy of the original, unattached Resource with the same attributes except the unique Resource UUID. Thus, we have two database records referencing the same file. We could accomplish something similar with a many-to-one mapping of Workspace to Resource s, but this was a choice we made which could allow for resource-copying if we ever allow file-editing in the future. In that case, attaching a Resource to a Workspace could create a copy of the file such that the original Resource remains unaltered. The user can, of course, change any of the mutable members of this new Workspace -associated Resource . The changes will be independent of the original \"unattached\" Resource . Users can remove a Resource from a Workspace if it has NOT been used for any portions of the analysis. We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Deletion of Resources Since multiple database records can reference the same underlying file, we have a bit of custom logic for determining when we delete only the database record versus deleting the actual underlying file. Essentially, if a deletion is requested and no other Resource database records reference the same file, then we delete both the database record AND the file. In the case where there is another database record referencing that file, we only remove the database record, leaving the file. Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). Some additional notes: Resource s are owned by users and can be added to a Workspace . However, that is not required-- Resource s can be \"unattached\". Regular users (non-admins) can't create new Resource directly via the API. The only way they can create a Resource is indirectly by adding a new upload. When a Resource is added to a Workspace , a new copy of the database record is made. This maintains the state of the original Resource . Resource s can be made \"public\" so that others can view and import them. Once another user chooses to import the file, a copy is made and that new user has their own copy. If a Resource is later made \"private\" then any files that have been \"used\" by others cannot be recalled. Resource s can be removed from a Workspace , but only if they have not been used for any analyses/operations. Resource s cannot be transferred from one Workspace to another, but they can be copied. A change in the type of the Resource can be requested. Until the validation of that change is complete, the Resource is made private and inactive. Admins can make essentially any change to Resources , including creation. However, they must be careful to maintain the integrity of the database and the files they point to. In a request to create a Resource via the API, the resource_type field can be blank/null. The type can be inferred from the path of the resource. We can do this because only admins are allowed to create via the API and they should only generate such requests if the resource type can be inferred (i.e. admins know not to give bad requests to the API...)","title":"Resources"},{"location":"setup_configuration/","text":"Configuration options and parameters As described in the installation section, the WebMEV API depends on environment variables passed to the container using the --env-file argument. Most variables defined in env_vars.template.txt should be appropriately commented, but we provide some additional instructions or commentary below. Email backends ( EMAIL_BACKEND_CHOICE ) Various operations like user registration and password changes require us to send users emails with encoded tokens to verify their email address and permit changes to their account. Accordingly, WebMEV needs the ability to send emails. We allow customization of this email backend through the EMAIL_BACKEND_CHOICE variable in env_vars.template.txt . Certain cloud providers, such as GCP, place restrictions on outgoing emails to prevent abuse. Per their documentation ( https://cloud.google.com/compute/docs/tutorials/sending-mail ), GCP recommends to use a third-party service such as SendGrid. If you wish to use these, you will have to implement your own email backend per the interface described at the Django project: https://docs.djangoproject.com/en/3.0/topics/email/#defining-a-custom-email-backend By default, we provide the following email backends: Console ( EMAIL_BACKEND_CHOICE=CONSOLE ) Note: This backend is for development purposes only-- no emails are actually sent! If EMAIL_BACKEND_CHOICE is not set, WebMEV defaults to using Django's \"console\" backend, which simply prints the emails to stdout. This is fine for development purposes where the tokens can be copy/pasted for live-testing, but is obviously not suitable for a production environment. Gmail ( EMAIL_BACKEND_CHOICE=GMAIL ) Alternatively, one can use the Gmail API to send emails from their personal or institution google account. In addition to setting EMAIL_BACKEND_CHOICE=GMAIL , you will need to set the following additional variables: GMAIL_ACCESS_TOKEN= GMAIL_REFRESH_TOKEN= GMAIL_CLIENT_ID= GMAIL_CLIENT_SECRET= These variables are obtained after you have created appropriate credentials and performed an Oauth2-based exchange, which we describe. Steps: Choose a Gmail account (or create one anew) from which you wish to send email notifications. On your own machine (or wherever you can login to your Google account), go to the Google developers console ( https://console.developers.google.com or https://console.cloud.google.com ) and head to \"APIs & Services\" and \"Dashboard\". Click on \"Enable APIs and Services\", search for \"Gmail\", and enable the Gmail API. Once that is enabled, go to the \"Credentials\" section under \"APIs and Services\". Just as above, we will create a set of OAuth credentials. Click on the \"Create credentials\" button and choose \"OAuth Client ID\". Choose \"Other\" from the options and give these credentials a name. Once the credentials are created, download the JSON-format file when it prompts. Using that credential file, run the helpers/exchange_gmail_credentials.py script like: python3 helpers/exchange_gmail_credentials.py -i <original_creds_path> -o <final_creds_path> (Note that this script can be run from within the WebMEV Docker container, as it contains the appropriate Python packages. If you are not using the application container, you can run the script as long as the google-auth-oauthlib library is installed-- https://pypi.org/project/google-auth-oauthlib/ ) The script will ask you to copy a link into your browser, which you can do on any machine where you can authenticate with Google. That URL will ask you to choose/log-in to the Gmail account you will be using to send emails. Finally, if successfully authenticated, it will provide you with a \"code\" which you will copy into your terminal. Once complete, the script will write a new JSON-format file at the location specified with the -o argument. Using the values in that final JSON file, copy/paste those into the file of environment variables you will be submitting to the WebMEV container upon startup. Be careful with these credentials as they give full access to the Gmail account in question!! Storage backends ( RESOURCE_STORAGE_BACKEND ) Storage of user files can be either local (on the MEV server) or in some remote filesystem (such as in a Google storage bucket). To abstract this, we have storage \"backends\" that control the behavior for each storage choice. Options for storage backends can be found in the api/storage_backends folder. To use a particular backend, we supply the \"dotted\" path for the class that implements the storage interface. For example, to use the Google bucket storage backend, we would use RESOURCE_STORAGE_BACKEND=api.storage_backends.google_cloud.GoogleBucketStorage since the GoogleBucketStorage class is located in api/storage_backends/google_cloud.py . Note that each storage backend may require additional environment variables to be set. We enforce this by attempting an initial import of the storage backend class. By convention, any configuration parameters required should at the \"top-level\" of the Python module/file. This way, when we attempt the import while starting the application, any missing configuration variables will raise an exception. This (hopefully) prevents errors in runtime due to incomplete/invalid configuration.","title":"Configuration"},{"location":"setup_configuration/#configuration-options-and-parameters","text":"As described in the installation section, the WebMEV API depends on environment variables passed to the container using the --env-file argument. Most variables defined in env_vars.template.txt should be appropriately commented, but we provide some additional instructions or commentary below.","title":"Configuration options and parameters"},{"location":"setup_configuration/#email-backends-email_backend_choice","text":"Various operations like user registration and password changes require us to send users emails with encoded tokens to verify their email address and permit changes to their account. Accordingly, WebMEV needs the ability to send emails. We allow customization of this email backend through the EMAIL_BACKEND_CHOICE variable in env_vars.template.txt . Certain cloud providers, such as GCP, place restrictions on outgoing emails to prevent abuse. Per their documentation ( https://cloud.google.com/compute/docs/tutorials/sending-mail ), GCP recommends to use a third-party service such as SendGrid. If you wish to use these, you will have to implement your own email backend per the interface described at the Django project: https://docs.djangoproject.com/en/3.0/topics/email/#defining-a-custom-email-backend By default, we provide the following email backends: Console ( EMAIL_BACKEND_CHOICE=CONSOLE ) Note: This backend is for development purposes only-- no emails are actually sent! If EMAIL_BACKEND_CHOICE is not set, WebMEV defaults to using Django's \"console\" backend, which simply prints the emails to stdout. This is fine for development purposes where the tokens can be copy/pasted for live-testing, but is obviously not suitable for a production environment. Gmail ( EMAIL_BACKEND_CHOICE=GMAIL ) Alternatively, one can use the Gmail API to send emails from their personal or institution google account. In addition to setting EMAIL_BACKEND_CHOICE=GMAIL , you will need to set the following additional variables: GMAIL_ACCESS_TOKEN= GMAIL_REFRESH_TOKEN= GMAIL_CLIENT_ID= GMAIL_CLIENT_SECRET= These variables are obtained after you have created appropriate credentials and performed an Oauth2-based exchange, which we describe. Steps: Choose a Gmail account (or create one anew) from which you wish to send email notifications. On your own machine (or wherever you can login to your Google account), go to the Google developers console ( https://console.developers.google.com or https://console.cloud.google.com ) and head to \"APIs & Services\" and \"Dashboard\". Click on \"Enable APIs and Services\", search for \"Gmail\", and enable the Gmail API. Once that is enabled, go to the \"Credentials\" section under \"APIs and Services\". Just as above, we will create a set of OAuth credentials. Click on the \"Create credentials\" button and choose \"OAuth Client ID\". Choose \"Other\" from the options and give these credentials a name. Once the credentials are created, download the JSON-format file when it prompts. Using that credential file, run the helpers/exchange_gmail_credentials.py script like: python3 helpers/exchange_gmail_credentials.py -i <original_creds_path> -o <final_creds_path> (Note that this script can be run from within the WebMEV Docker container, as it contains the appropriate Python packages. If you are not using the application container, you can run the script as long as the google-auth-oauthlib library is installed-- https://pypi.org/project/google-auth-oauthlib/ ) The script will ask you to copy a link into your browser, which you can do on any machine where you can authenticate with Google. That URL will ask you to choose/log-in to the Gmail account you will be using to send emails. Finally, if successfully authenticated, it will provide you with a \"code\" which you will copy into your terminal. Once complete, the script will write a new JSON-format file at the location specified with the -o argument. Using the values in that final JSON file, copy/paste those into the file of environment variables you will be submitting to the WebMEV container upon startup. Be careful with these credentials as they give full access to the Gmail account in question!!","title":"Email backends (EMAIL_BACKEND_CHOICE)"},{"location":"setup_configuration/#storage-backends-resource_storage_backend","text":"Storage of user files can be either local (on the MEV server) or in some remote filesystem (such as in a Google storage bucket). To abstract this, we have storage \"backends\" that control the behavior for each storage choice. Options for storage backends can be found in the api/storage_backends folder. To use a particular backend, we supply the \"dotted\" path for the class that implements the storage interface. For example, to use the Google bucket storage backend, we would use RESOURCE_STORAGE_BACKEND=api.storage_backends.google_cloud.GoogleBucketStorage since the GoogleBucketStorage class is located in api/storage_backends/google_cloud.py . Note that each storage backend may require additional environment variables to be set. We enforce this by attempting an initial import of the storage backend class. By convention, any configuration parameters required should at the \"top-level\" of the Python module/file. This way, when we attempt the import while starting the application, any missing configuration variables will raise an exception. This (hopefully) prevents errors in runtime due to incomplete/invalid configuration.","title":"Storage backends (RESOURCE_STORAGE_BACKEND)"},{"location":"workspaces/","text":"Workspaces class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"},{"location":"workspaces/#workspaces","text":"class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"}]}